# Plot the elbow graph
plot(1:10, ssd, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)", ylab = "Total Within Sum of Squares")
# Add a line at the elbow
abline(v = 3, col = "red", lty = 2)  # Change 3 to the chosen k
# Plot the elbow graph
plot(1:10, ssd, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)", ylab = "Total Within Sum of Squares")
# Add a line at the elbow
abline(v = 4, col = "red", lty = 2)  # Change 3 to the chosen k
# Plot the elbow graph
plot(1:10, ssd, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters (k)", ylab = "Total Within Sum of Squares")
# Add a line at the elbow
abline(v = 3, col = "red", lty = 2)  # Change 3 to the chosen k
# Choose the number of clusters using the elbow method or other techniques
k <- 3  # replace with the chosen number of clusters
# Apply k-means clustering
kmeans_result <- kmeans(scaled_data, centers = k, nstart = 25)  # nstart for more robust results
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(. ~ Cluster, data = pharma_data[, c(1:10)], mean)
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(. ~ Cluster, data = pharma_data[, c("Cluster", "Market_Cap", "Beta", "PE_Ratio", "ROE", "ROA", "Asset_Turnover", "Leverage", "Revenue_Growth", "Net_Profit_Margin")], mean)
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(. , data = pharma_data[, c("Cluster", "Market_Cap", "Beta", "PE_Ratio", "ROE", "ROA", "Asset_Turnover", "Leverage", "Revenue_Growth", "Net_Profit_Margin")], mean)
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(.~, data = pharma_data[, c("Cluster", "Market_Cap", "Beta", "PE_Ratio", "ROE", "ROA", "Asset_Turnover", "Leverage", "Revenue_Growth", "Net_Profit_Margin")], mean)
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(.~ data = pharma_data[, c("Cluster", "Market_Cap", "Beta", "PE_Ratio", "ROE", "ROA", "Asset_Turnover", "Leverage", "Revenue_Growth", "Net_Profit_Margin")], mean)
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(.~ data[, c("Cluster", "Market_Cap", "Beta", "PE_Ratio", "ROE", "ROA", "Asset_Turnover", "Leverage", "Revenue_Growth", "Net_Profit_Margin")], mean)
# Choose the number of clusters using the elbow method or other techniques
k <- 3  # replace with the chosen number of clusters
# Apply k-means clustering
kmeans_result <- kmeans(scaled_data, centers = k, nstart = 25)  # nstart for more robust results
# Choose the number of clusters using the elbow method or other techniques
k <- 3  # replace with the chosen number of clusters
# Apply k-means clustering
kmeans_result <- kmeans(scaled_data, centers = k, nstart = 25)  # nstart for more robust results
kmeans_result
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(.~cluster, data[, c("Cluster", "Market_Cap", "Beta", "PE_Ratio", "ROE", "ROA", "Asset_Turnover", "Leverage", "Revenue_Growth", "Net_Profit_Margin")], mean)
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(. ~ Cluster, data = numerical_data[, c(1:9, "Cluster")], mean)
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(. ~ cluster, data = numerical_data[, c(1:9, "cluster")], mean)
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(. ~ cluster, data = pharma_data[, c("cluster", "Market_Cap", "Beta", "PE_Ratio", "ROE", "ROA", "Asset_Turnover", "Leverage", "Revenue_Growth", "Net_Profit_Margin")], mean)
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(. ~ cluster, data = pharma_data[, c("cluster", "centers",  "totss" , "withinss" , "tot.withinss", "betweenss", "size", "iter" , "ifault")], mean)
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(. ~ Cluster, data = pharma_data[, c("Cluster", "centers", "totss", "withinss", "tot.withinss", "betweenss", "size", "iter", "ifault")], mean)
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(. ~ Cluster, data = pharma_data[, c("Cluster", "Market_Cap", "Beta", "PE_Ratio", "ROE", "ROA", "Asset_Turnover", "Leverage", "Revenue_Growth", "Net_Profit_Margin")], mean)
kmeans_result.columns
ssd.columns
# Assuming 'pharma_data' is your data frame
column_names <- colnames(pharma_data)
# Print the column names
print(column_names)
# Assuming 'pharma_data' is your data frame
column_names <- colnames(ssd)
# Print the column names
print(column_names)
# Assuming 'pharma_data' is your data frame
column_names <- colnames(numerical_data)
# Print the column names
print(column_names)
# Add cluster assignments to the original data
pharma_data$Cluster <- kmeans_result$cluster
# Analyze clusters
cluster_summary <- aggregate(. ~ Cluster, data = pharma_data[, c("Cluster", "Market_Cap", "Beta", "PE_Ratio", "ROE", "ROA", "Asset_Turnover", "Leverage")], mean)
# Interpret clusters with respect to numerical variables
print(cluster_summary)
# Naming clusters using non-numerical variables
for (cluster_num in unique(pharma_data$Cluster)) {
cluster_name <- names(sort(table(pharma_data$Location[pharma_data$Cluster == cluster_num]), decreasing = TRUE))[1]
cat("Cluster", cluster_num, ":", cluster_name, "\n")
}
library(ISLR)
library(rpart)
library(rpart.plot)
MyData <- Carseats[,1:8]
Model_1 =rpart (Sales~.,data=MyData, method= 'anova')
View(MyData)
library(ISLR)
library(rpart)
library(rpart.plot)
MyData <- Carseats[,1:8]
Model_1 =rpart (Sales~.,data=MyData, method= 'anova')
#Load the libraries
library(tidyverse, warn.conflicts = FALSE)
library(factoextra,warn.conflicts = FALSE)
library (caret, warn.conflicts = FALSE)
library (e1071,warn.conflicts = FALSE)
library(cluster,warn.conflicts = FALSE)
library(dplyr,warn.conflicts = FALSE)
library(tinytex,warn.conflicts = FALSE)
library(dbscan,warn.conflicts = FALSE)
library(fpc,warn.conflicts = FALSE)
df = read.csv("fuel_receipts1.csv")
str(df)
summary(df)
head(df)
# Assuming df is your dataset
selected_columns <- df[, -c(1,2,4,5, 6, 7)]
#Scaling
z = df[, -c(1,2,4,5,6, 7)]
df_scale = scale(z)
head(df_scale)
library(ISLR)
library(dplyr)
library(glmnet)
library(caret)
Carseats_Filtered <- Carseats %>% select(Sales, Price, Advertising, Population, Age, Income, Education)
#Filter the relevant attributes
Carseats_Filtered <- Carseats %>% select(Sales, Price, Advertising, Population, Age, Income, Education)
#Preprocess the data(scale and center the features)
preProcess_data <- preProcess(Carseats_Filtered[, -1], method = c("center", "scale"))
Carseats_Scaled <- predict(preProcess_data, Carseats_Filtered[, -1])
# Convert predictors to matrix
X <- as.matrix(Carseats_Scaled)
# Response variable (Sales)
y <- Carseats_Filtered$Sales
# Lasso regression with cross-validation
lasso_model <- cv.glmnet(X, y, alpha = 1)
# Best lambda value
best_lambda <- lasso_model$lambda.min
print(best_lambda)
#QB2. What is the coefficient for the price (normalized) attribute in the best model (i.e. model
#with the optimal lambda)?
# Get the coefficients of the model with the optimal lambda
lasso_coefficients <- coef(lasso_model, s = best_lambda)
# Convert the coefficients to a readable format
coefficients_df <- as.data.frame(as.matrix(lasso_coefficients))
# Print the coefficient for Price (normalized)
price_coefficient <- coefficients_df["Price", ]
print(price_coefficient)
#QB3. How many attributes remain in the model if lambda is set to 0.01? How that number
#changes if lambda is increased to 0.1? Do you expect more variables to stay in the model (i.e., to
have non-zero coefficients) as we increase lambda?
#QB3. How many attributes remain in the model if lambda is set to 0.01? How that number
#changes if lambda is increased to 0.1? Do you expect more variables to stay in the model
#(i.e., thave non-zero coefficients) as we increase lambda?
# Get the coefficients for lambda = 0.01
lasso_coefficients_0.01 <- coef(lasso_model, s = 0.01)
# Count the number of non-zero coefficients
non_zero_coeffs_0.01 <- sum(lasso_coefficients_0.01 != 0)
print(non_zero_coeffs_0.01)
# Get the coefficients for lambda = 0.1
lasso_coefficients_0.1 <- coef(lasso_model, s = 0.1)
# Count the number of non-zero coefficients
non_zero_coeffs_0.1 <- sum(lasso_coefficients_0.1 != 0)
print(non_zero_coeffs_0.1)
#QB4. Build an elastic-net model with alpha set to 0.6.
#What is the best value of lambda for such a model?
# Elastic-Net model with alpha = 0.6 and cross-validation
elastic_net_model <- cv.glmnet(X, y, alpha = 0.6)
# Best lambda for the Elastic-Net model
best_lambda_elastic_net <- elastic_net_model$lambda.min
print(best_lambda_elastic_net)
knitr::opts_chunk$set(echo = TRUE)
#Load necessary libraries
library(ISLR)
library(dplyr)
library(glmnet)
library(caret)
#Filter the relevant attributes
Carseats_Filtered <- Carseats %>% select(Sales, Price, Advertising, Population, Age, Income, Education)
#Preprocess the data(scale and center the features)
preProcess_data <- preProcess(Carseats_Filtered[, -1], method = c("center", "scale"))
Carseats_Scaled <- predict(preProcess_data, Carseats_Filtered[, -1])
# Convert predictors to matrix
X <- as.matrix(Carseats_Scaled)
# Response variable (Sales)
y <- Carseats_Filtered$Sales
# Lasso regression with cross-validation
lasso_model <- cv.glmnet(X, y, alpha = 1)
# Best lambda value
best_lambda <- lasso_model$lambda.min
print(best_lambda)
#Filter the relevant attributes
Carseats_Filtered <- Carseats %>% select(Sales, Price, Advertising, Population, Age, Income, Education)
#Preprocess the data(scale and center the features)
preProcess_data <- preProcess(Carseats_Filtered[, -1], method = c("center", "scale"))
Carseats_Scaled <- predict(preProcess_data, Carseats_Filtered[, -1])
# Convert predictors to matrix
X <- as.matrix(Carseats_Scaled)
# Response variable (Sales)
y <- Carseats_Filtered$Sales
# Lasso regression with cross-validation
lasso_model <- cv.glmnet(X, y, alpha = 1)
# Best lambda value
best_lambda <- lasso_model$lambda.min
print(best_lambda)
# Get the coefficients of the model with the optimal lambda
lasso_coefficients <- coef(lasso_model, s = best_lambda)
# Convert the coefficients to a readable format
coefficients_df <- as.data.frame(as.matrix(lasso_coefficients))
# Print the coefficient for Price (normalized)
price_coefficient <- coefficients_df["Price", ]
print(price_coefficient)
# Get the coefficients for lambda = 0.01
lasso_coefficients_0.01 <- coef(lasso_model, s = 0.01)
# Count the number of non-zero coefficients
non_zero_coeffs_0.01 <- sum(lasso_coefficients_0.01 != 0)
print(non_zero_coeffs_0.01)
# Get the coefficients for lambda = 0.1
lasso_coefficients_0.1 <- coef(lasso_model, s = 0.1)
# Count the number of non-zero coefficients
non_zero_coeffs_0.1 <- sum(lasso_coefficients_0.1 != 0)
print(non_zero_coeffs_0.1)
# Get the coefficients for lambda = 0.01
lasso_coefficients_0.01 <- coef(lasso_model, s = 0.01)
# Count the number of non-zero coefficients
non_zero_coeffs_0.01 <- sum(lasso_coefficients_0.01 != 0)
print(non_zero_coeffs_0.01)
# Get the coefficients for lambda = 0.1
lasso_coefficients_0.1 <- coef(lasso_model, s = 0.1)
# Count the number of non-zero coefficients
non_zero_coeffs_0.1 <- sum(lasso_coefficients_0.1 != 0)
print(non_zero_coeffs_0.1)
# Elastic-Net model with alpha = 0.6 and cross-validation
elastic_net_model <- cv.glmnet(X, y, alpha = 0.6)
# Best lambda for the Elastic-Net model
best_lambda_elastic_net <- elastic_net_model$lambda.min
print(best_lambda_elastic_net)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(dplyr)
library(glmnet)
library(caret)
Carseats_Filtered <- Carseats %>% select("Sales", "Price",
"Advertising","Population","Age","Income","Education")
# First, let's make sure we have the rpart package
if (!require(rpart)) install.packages("rpart")
library(rpart)
# Build the decision tree model
tree_model <- rpart(Sales ~ ., data = Carseats_Filtered)
# Print the summary of the model
summary(tree_model)
# Plot the tree
plot(tree_model)
text(tree_model, pretty = 0)
# Create a new data frame with the given input
new_data <- data.frame(
Price = 6.54,
Population = 124,
Advertising = 0,
Age = 76,
Income = 110,
Education = 10
)
# Use the tree model to predict Sales for the new data
predicted_sales <- predict(tree_model, newdata = new_data)
# Print the predicted Sales
print(predicted_sales)
# Set the random seed for reproducibility
set.seed(123)
# Train the random forest model using caret
rf_model <- train(Sales ~ .,
data = Carseats_Filtered,
method = "rf",
trControl = trainControl(method = "cv", number = 5))
# Print the results
print(rf_model)
# Set the random seed for reproducibility
set.seed(123)
# Define the custom search grid
custom_grid <- expand.grid(mtry = c(2, 3, 5))
# Define the control parameters for training
ctrl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 3)
# Train the random forest model with the custom grid
rf_model_custom <- train(Sales ~ .,
data = Carseats_Filtered,
method = "rf",
metric = "RMSE",
tuneGrid = custom_grid,
trControl = ctrl)
# Print the results
print(rf_model_custom)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
data <- read_csv(""C:\Users\samyu\Downloads\train_v3.csv"")
library(readr)
data <- read_csv("C:\Users\samyu\Downloads\train_v3.csv")
library(readr)
data <- read_csv("C:\Users\samyu\Downloads\train_v3.csv")
library(readr)
data <- read_csv("C:\Users\samyu\Downloads\train_v3.csv")
library(readr)
data <- read_csv("C:/Users/samyu/Downloads/train_v3.csv")
spec(data)
# Check for missing values in the entire dataset
missing_values <- is.na(data)
# View the missing values matrix
head(missing_values)
# View columns with missing values
columns_with_missing <- data[, colSums(is.na(data)) > 0]
head(columns_with_missing)
# View columns with missing values
columns_with_missing <- data[, colSums(is.na(data)) > 0]
# View columns with missing values
columns_with_missing <- data[, colSums(is.na(data)) > 0]
# Check if any column contains character or factor values
is_character_or_factor <- sapply(data, function(x) is.character(x) | is.factor(x))
# View which columns are character or factor
is_character_or_factor
# View columns with missing values
columns_with_missing <- data[, colSums(is.na(data)) > 0]
# View columns with missing values
columns_with_missing <- data[, colSums(is.na(data)) > 0]
columns_with_missing
# View columns with missing values
columns_with_missing <- data[, colSums(is.na(data)) > 0]
print(columns_with_missing)
sum(is.na(data))  # Check total number of missing values in the entire dataset
library(DMwR)
install.packages("DMwR2")
library(DMwR2)
# Perform KNN imputation (you can specify k, the number of neighbors)
data <- knnImputation(data, k = 5)  # k = 5 is a common choice
install.packages("DMwR2")
library(DMwR2)
# Perform KNN imputation (you can specify k, the number of neighbors)
data <- knnImputation(data, k = 3)  # k = 5 is a common choice
knitr::opts_chunk$set(echo = TRUE)
# Check if any column contains character or factor values
is_character_or_factor <- sapply(data, function(x) is.character(x) | is.factor(x))
# View which columns are character or factor
is_character_or_factor
install.packages("DMwR2")
library(DMwR2)
# Perform KNN imputation (you can specify k, the number of neighbors)
data <- knnImputation(data, k = 3)  # k = 5 is a common choice
install.packages("mice")
library(mice)
install.packages("mice")
library(mice)
imputed_data <- mice(data, method = 'pmm', m = 5)
(is.na(data))  # Check total number of missing values in the entire dataset
count(is.na(data))  # Check total number of missing values in the entire dataset
is.na(data)
# Check if any column contains character or factor values
is_character_or_factor <- sapply(data, function(x) is.character(x) | is.factor(x))
# View which columns are character or factor
is_character_or_factor
# Check if any column contains character or factor values
is_character_or_factor <- sapply(data, function(x) is.character(x) | is.factor(x))
# View which columns are character or factor
is_character_or_factor
table(is_character_or_factor)
# Check if any column contains character or factor values
is_character_or_factor <- sapply(data, function(x) is.character(x) | is.factor(x))
# View which columns are character or factor
table(is_character_or_factor)
str(data)
table(is.na(data))
is.na(data)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(class)
library(ISLR)
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(gmodels)
library(modeest)
library(readr)
library(class)
library(ISLR)
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(gmodels)
library(modest)
library(readr)
library(class)
library(ISLR)
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(gmodels)
library(modeest)
library(readr)
library(class)
library(ISLR)
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(gmodels)
library(modeest)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(class)
library(ISLR)
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(gmodels)
library(modeest)
library(readr)
library(class)
library(ISLR)
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(gmodels)
library (modeest)
install.packages("modeest")
library(readr)
library(class)
library(ISLR)
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(gmodels)
library (modeest)
install.packages("modeest")
install.packages("modeest")
install.packages("modeest")
install.packages("modeest")
install.packages("modeest")
install.packages("modeest")
install.packages("modeest")
install.packages("modeest")
knitr::opts_chunk$set(echo = TRUE)
install.packages("modeest")
library(readr)
library(class)
library(ISLR)
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(gmodels)
library (modeest)
library(moments)
install.packages("modeest")
library(readr)
library(class)
library(ISLR)
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(gmodels)
library (modeest)
install.packages("moments")
knitr::opts_chunk$set(echo = TRUE)
install.packages("modeest")
library(readr)
library(class)
library(ISLR)
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(gmodels)
library (modeest)
install.packages("moments")
library(moments)
library(ggcorrplot)
install.packages("modeest")
library(readr)
library(class)
library(ISLR)
library(dplyr)
library(ggplot2)
library(caret)
library(pROC)
library(gmodels)
library (modeest)
install.packages("moments")
library(moments)
install.packages("ggcorrplot")
Sys.which("make")
